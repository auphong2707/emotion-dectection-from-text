{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nj7DW4_mFDBh"
      },
      "source": [
        "# **Data Preprocess**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDTriPXukNqb"
      },
      "source": [
        "## Importation & Data loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKqQ0TvIZSOK"
      },
      "source": [
        "Preparing necessary packages (may need to add more):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9TbRS-hBXsHe"
      },
      "outputs": [],
      "source": [
        "import numpy as np  # Work with multi-dimenional data\n",
        "import pandas as pd # Work with relational data\n",
        "import matplotlib.pyplot as plt # Visualize data\n",
        "import seaborn as sns # Visualize data base on matplotlib\n",
        "import nltk #\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from scipy import sparse\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTgWau1hdl23"
      },
      "source": [
        "Load data from files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BY138Gindon3",
        "outputId": "20e0b050-60cd-4a03-ffc8-2a8cd393aa5b"
      },
      "outputs": [],
      "source": [
        "directory = 'data/dataset/raw/'\n",
        "\n",
        "# This method use to extract all the file in the input list\n",
        "def extract_data(files):\n",
        "  data_x_raw = list()\n",
        "  data_y_raw = list()\n",
        "  for file in files:\n",
        "    with open(directory + file) as f:\n",
        "      for line in f:\n",
        "        line = line.strip('\\n')\n",
        "        x_raw, y_raw = line.split(sep=';')\n",
        "\n",
        "        data_x_raw.append(x_raw)\n",
        "        data_y_raw.append(y_raw)\n",
        "\n",
        "  return data_x_raw, data_y_raw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get all data\n",
        "\n",
        "X_raw, y_raw = extract_data(['train.txt', 'val.txt', 'test.txt'])\n",
        "X_train_raw, X_test_raw, y_train_raw, y_test_raw = train_test_split(X_raw, y_raw, test_size = 0.2)\n",
        "\n",
        "print(\"Traning data's size is:\", len(X_train_raw))\n",
        "print(\"Test data's size is:\", len(X_test_raw))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UhsCl8HgxKV"
      },
      "source": [
        "## Inital feature Extraction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkacoVpjhG6O"
      },
      "source": [
        "Removing the stopword"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0OCse4nNgaoy"
      },
      "outputs": [],
      "source": [
        "file = open(\"data/stopwords/stop_words_english.txt\", 'r', encoding='utf-8')\n",
        "stopword_list = file.read().split('\\n')\n",
        "file.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRGC3YwhD6Qx"
      },
      "source": [
        "Vectorize the data:\n",
        "\n",
        "1.   Bag Of Words model (https://en.wikipedia.org/wiki/Bag-of-words_model)\n",
        "2.   BoW/TF-IDF model (https://en.wikipedia.org/wiki/Tf%E2%80%93idf)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YY6UaDwAD_Aj",
        "outputId": "1a4102e0-7a25-4fcd-8804-cd327552d601"
      },
      "outputs": [],
      "source": [
        "# Scikit CountVectorizer:\n",
        "# Documentation: (https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)\n",
        "\n",
        "count_vector = CountVectorizer(stop_words=stopword_list, ngram_range=(1, 2))\n",
        "count_vector.fit(X_train_raw, X_test_raw)\n",
        "X_train_bow = count_vector.transform(X_train_raw)\n",
        "X_test_bow = count_vector.transform(X_test_raw)\n",
        "dictionary = count_vector.get_feature_names_out()\n",
        "\n",
        "print(\"Shape of preprocessed training data X using BoW model is: \", X_train_bow.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D3a48rJjEHcG",
        "outputId": "29671b0a-cee6-4a0e-bc05-54348d303d64"
      },
      "outputs": [],
      "source": [
        "# Scikit TF-IDF:\n",
        "# Documentation: (https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer)\n",
        "\n",
        "tfidf_vector = TfidfVectorizer(stop_words=stopword_list, ngram_range=(1, 2))\n",
        "tfidf_vector.fit(X_train_raw, X_test_raw)\n",
        "X_train_tfidf = tfidf_vector.transform(X_train_raw)\n",
        "X_test_tfidf = tfidf_vector.transform(X_test_raw)\n",
        "\n",
        "print(\"Shape of preprocessed training data X using BoW/TF-IDF model is: \", X_train_tfidf.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyttkaIPIuTR"
      },
      "source": [
        "Cast output list to ndarray"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSQpqMqlI0Z9",
        "outputId": "efbcea7b-dd4c-4c1a-fc0d-308dd6a2603b"
      },
      "outputs": [],
      "source": [
        "y_train = np.asarray(y_train_raw)\n",
        "y_test = np.asarray(y_test_raw)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ob9acTnuVLG"
      },
      "source": [
        "*Luckily we're quite done with the abbreviations!*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqwxPqeLvqH8"
      },
      "source": [
        "Another way: Using Word2Vec to capture the semantics\n",
        "(Um, we'd talk about that later, ok?)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uS4maCj-vvCe"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Fhcc_aUTueC"
      },
      "source": [
        "## Initial exploratory Data Analysis (EDA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2B0kWS9bktj2"
      },
      "source": [
        "Average length of the comment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3n36QobPkewe",
        "outputId": "9c647833-7410-4f04-816c-be69413b7959"
      },
      "outputs": [],
      "source": [
        "# Calculate average number of characters\n",
        "sum_of_chars = sum([len(x) for x in X_train_raw])\n",
        "mean_number_chars = sum_of_chars / len(X_train_raw)\n",
        "\n",
        "# Calculate average number of words\n",
        "sum_of_words = sum([len(x.split()) for x in X_train_raw])\n",
        "mean_number_words = sum_of_words / len(X_train_raw)\n",
        "\n",
        "# Print\n",
        "print('The mean number of characters of each line is: %d' % mean_number_chars)\n",
        "print('The mean number of words of each line is: %d' % mean_number_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAZxbqwQpJ_H"
      },
      "source": [
        "Statistic of output labels:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 563
        },
        "id": "uqs1jJTspQX5",
        "outputId": "d5cdb852-87d4-4c02-fcaa-9674d73a02e9"
      },
      "outputs": [],
      "source": [
        "# Number of labels:\n",
        "labels = np.unique(y_train)\n",
        "print('Number of labels is: %d' % len(labels))\n",
        "print('Labels: ' + ', '.join(labels))\n",
        "print('\\n----------------------------------------------------------------------------\\n')\n",
        "\n",
        "# Plot:\n",
        "df = pd.DataFrame(\n",
        "  dict(\n",
        "    labels = labels,\n",
        "    label_elements_count = [y_train_raw.count(label) for label in labels]\n",
        "  )\n",
        ")\n",
        "df = df.sort_values('label_elements_count')\n",
        "\n",
        "plt.title(label='Labels statistics')\n",
        "plt.xlabel('Labels')\n",
        "plt.ylabel('Count')\n",
        "\n",
        "plt.bar('labels', 'label_elements_count', data=df)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNNYNBrIts1k"
      },
      "source": [
        "Statistics of words with the highest frequency of each label:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eieb8hWmtsT1"
      },
      "outputs": [],
      "source": [
        "def show_highest_frequency_words():\n",
        "  vect_arr = X_train_bow.toarray()\n",
        "  vect_aggr = np.zeros(shape=(len(labels), vect_arr.shape[1]))\n",
        "\n",
        "  for idx in range(len(vect_arr)):\n",
        "    label_idx = int(np.where(labels==y_train[idx])[0][0])\n",
        "\n",
        "    vect_aggr[label_idx] += vect_arr[idx]\n",
        "\n",
        "  fig, axs = plt.subplots(3, 2)\n",
        "\n",
        "  for label_idx in range(len(labels)):\n",
        "    axs_x, axs_y = label_idx // 2, label_idx % 2\n",
        "\n",
        "    plt_ref = axs[axs_x][axs_y]\n",
        "    plt_ref.set_title(labels[label_idx].upper())\n",
        "\n",
        "    x_axis, y_axis = list(), list()\n",
        "    n_argmax = np.argpartition(vect_aggr[label_idx], -7)[-7:]\n",
        "    n_argmax = n_argmax[np.argsort(vect_aggr[label_idx][n_argmax])]\n",
        "    for i in n_argmax:\n",
        "      x_axis.append(dictionary[i])\n",
        "      y_axis.append(vect_aggr[label_idx][i])\n",
        "\n",
        "    x_axis = np.asarray(x_axis)\n",
        "    y_axis = np.asarray(y_axis)\n",
        "\n",
        "    plt_ref.barh(x_axis, y_axis)\n",
        "\n",
        "  # Adjust layout\n",
        "  plt.tight_layout()\n",
        "\n",
        "  # Show the plot\n",
        "  plt.show()\n",
        "\n",
        "show_highest_frequency_words()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Fixing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This part will eliminate all the words that have bad influence to our data.\n",
        "All the words will be stored in a file stored in file *filtered_words.txt*\n",
        "\n",
        "* Words appears so many times but don't contribute much (*feel*, *feeling*,...).\n",
        "* Words have no mean (*aa*,*ab*,...)\n",
        "* Words appears only a few"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "file = open(\"data/stopwords/filtered_words.txt\", 'r', encoding='utf-8')\n",
        "filtered_words = file.read().split('\\n')\n",
        "\n",
        "file = open(\"data/stopwords/stop_words_english.txt\", 'r', encoding='utf-8')\n",
        "stop_words = file.read().split('\\n')\n",
        "\n",
        "new_stopwords = stopword_list + stop_words + filtered_words\n",
        "unfiltered_stopwords = stop_words + stopword_list\n",
        "\n",
        "print(len(new_stopwords), len(unfiltered_stopwords))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create tokenize function with the help of nltk packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "stemmer = PorterStemmer()\n",
        "\n",
        "def tokenize(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [word for word in tokens if word not in new_stopwords]\n",
        "    stems = [stemmer.stem(token) for token in tokens]\n",
        "    return stems\n",
        "\n",
        "def unfiltered_tokenize(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [word for word in tokens if word not in unfiltered_stopwords]\n",
        "    stems = [stemmer.stem(token) for token in tokens]\n",
        "    return stems"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fit the data again"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dictionary semi_filtered: Using basic stopwords only\n",
        "\n",
        "count_vector = CountVectorizer(stop_words=unfiltered_stopwords, tokenizer=unfiltered_tokenize, ngram_range=(1, 2))\n",
        "count_vector.fit(X_train_raw, X_test_raw)\n",
        "X_train_bow_semi_filtered = count_vector.transform(X_train_raw)\n",
        "X_test_bow_semi_filtered = count_vector.transform(X_test_raw)\n",
        "dictionary_semi_filtered = count_vector.get_feature_names_out()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dictionary semi_mono: Using basic filters, containing only 1-gram, used to perform LASSO-filter to obtain L1 dict\n",
        "\n",
        "count_vector = CountVectorizer(stop_words=unfiltered_stopwords, tokenizer=unfiltered_tokenize, ngram_range=(1, 1))\n",
        "count_vector.fit(X_train_raw, X_test_raw)\n",
        "X_train_bow_semi_mono = count_vector.transform(X_train_raw)\n",
        "X_test_bow_semi_mono = count_vector.transform(X_test_raw)\n",
        "dictionary_semi_mono = count_vector.get_feature_names_out()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sorting out the useless stopwords to make L1 dictionary. We perform LASSO Logistic Regression on dictionary_semi_mono to sort out all useless stopwords (criterion: L2 norm of the L1 weight = 0), then make it a new stopword list.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "linear_semi_mono = LogisticRegression(penalty='l1', solver='saga')\n",
        "linear_semi_mono.fit(X_train_bow_semi_mono, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(X_train_bow_semi_mono.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(linear_semi_mono.coef_.shape)\n",
        "lin_coef = linear_semi_mono.coef_.T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def norm2(a: tuple):\n",
        "    res = 0\n",
        "    for i in a:\n",
        "        res += i**2\n",
        "    return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "useless_coef = list()\n",
        "for i in range(lin_coef.shape[0]):\n",
        "    if norm2(lin_coef[i]) == 0:\n",
        "        useless_coef.append(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(len(useless_coef))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(\"data/stopwords/useless.txt\", 'w') as file:\n",
        "    for i in useless_coef:\n",
        "        file.write(dictionary_semi_mono[i])\n",
        "        file.write('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "file = open(\"data/stopwords/useless.txt\", 'r', encoding='utf-8')\n",
        "useless = file.read().split('\\n')\n",
        "\n",
        "L1_stopwords = unfiltered_stopwords + useless"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def useless_tokenize(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [word for word in tokens if word not in L1_stopwords]\n",
        "    stems = [stemmer.stem(token) for token in tokens]\n",
        "    return stems"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dictionary L1: Obtained by selecting useful tokens from semi_mono using L1 decomposition.\n",
        "\n",
        "count_vector = CountVectorizer(stop_words=L1_stopwords, tokenizer=useless_tokenize, ngram_range=(1, 2))\n",
        "count_vector.fit(X_train_raw, X_test_raw)\n",
        "X_train_bow_L1 = count_vector.transform(X_train_raw)\n",
        "X_test_bow_L1 = count_vector.transform(X_test_raw)\n",
        "dictionary_L1 = count_vector.get_feature_names_out()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(len(dictionary_semi_filtered), len(dictionary_L1), len(dictionary_semi_mono))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fixing results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "show_highest_frequency_words()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Export data to files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "save_directory = \"data/dataset/processed/\"\n",
        "\n",
        "sparse.save_npz(save_directory + \"X_train_bow.npz\", X_train_bow)\n",
        "sparse.save_npz(save_directory + \"X_test_bow.npz\", X_test_bow)\n",
        "\n",
        "sparse.save_npz(save_directory + \"X_train_tfidf.npz\", X_train_tfidf)\n",
        "sparse.save_npz(save_directory + \"X_test_tfidf.npz\", X_test_tfidf)\n",
        "\n",
        "np.savetxt(save_directory + \"y_train.txt\", y_train, fmt='%s')\n",
        "np.savetxt(save_directory + \"y_test.txt\", y_test, fmt='%s')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9syl2WgjTppC"
      },
      "source": [
        "## Variables summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvyhe6bnT0at"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**X_train_raw**, **y_train_raw**: ***list***\n",
        "\n",
        "Raw training data which is extracted directly from files.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**X_test**, **y_test**: ***list***\n",
        "\n",
        "  Raw testing data which is extracted directly from files.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**stopword_list**: ***list***\n",
        "\n",
        " List of stop words used in BoW and BoW/TF-IDF models\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**dictionary**: ***numpy.ndarray***\n",
        "\n",
        " Dictionary of words which is used to vectorized the data and number of occurences.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**X_train_bow**: ***scipy.sparse._csr.csr_matrix***\n",
        "\n",
        " Training data input which is vectorized by the BoW model.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**X_train_tfidf**: ***scipy.sparse._csr.csr_matrix***\n",
        "\n",
        " Training data input which is vectorized by the BoW/TFIDF model\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**X_test_bow**: ***scipy.sparse._csr.csr_matrix***\n",
        "\n",
        " Testing data input which is vectorized by the BoW model.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**X_test_tfidf**: ***scipy.sparse._csr.csr_matrix***\n",
        "\n",
        " Testing data input which is vectorized by the BoW/TFIDF model\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**y_train**: ***numpy.ndarray***\n",
        "\n",
        " Preprocessed traning data output\n",
        "\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
