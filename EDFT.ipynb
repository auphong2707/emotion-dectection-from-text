{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/auphong2707/emotion-dectection-from-text/blob/preprocess-refactoring/EDFT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction"
      ],
      "metadata": {
        "id": "AHEsUwSBCHbK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The project aims to create a **Machine Learning** powered model which is capable of classifying the emotions of the text (in the form of short quotes/comments) with predefined labels of emotions."
      ],
      "metadata": {
        "id": "FRPrD_ccFVz9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Emotion detection, also known as sentiment analysis, is now one of the most attractive subfields of Machine Learning, especially Natural Language Processing (NLP) due to its wide applications in many aspects of modern life, such as providing emotional information to help people developing insights or making decisions. In this project, we specifically try to convey the main emotion of a comment sentence. Generally, we expect the model to be able to take the input sentence in the form of short comments and returns its major emotion(s), corresponding to one of the predefined labels."
      ],
      "metadata": {
        "id": "TMh_xASYG7cZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our approach to solving this problem is to split it into two halves:\n",
        "\n",
        "  1.   Data Preprocessing\n",
        "  2.   Model training and Result\n",
        "\n",
        "For detail of each half, please go into the section\n"
      ],
      "metadata": {
        "id": "T65Wg0STHA3s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset we use in this project is obtained from Kaggle: [EDFT Dataset](https://www.kaggle.com/datasets/praveengovi/emotions-dataset-for-nlp)"
      ],
      "metadata": {
        "id": "fDK4XIuNIgMl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Connect to Google Drive"
      ],
      "metadata": {
        "id": "6CJSmsQlUZJt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Preprocessing**"
      ],
      "metadata": {
        "id": "nj7DW4_mFDBh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importation & Data loading"
      ],
      "metadata": {
        "id": "aDTriPXukNqb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "2w5klLSgUYgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preparing necessary packages (may need to add more):"
      ],
      "metadata": {
        "id": "NKqQ0TvIZSOK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np  # Work with multi-dimenional data\n",
        "import pandas as pd # Work with relational data\n",
        "import matplotlib.pyplot as plt # Visualize data\n",
        "import seaborn as sns # Visualize data base on matplotlib\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "# Because I hate seeing stupid warnings~\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
        "# Used for automatic feature extraction\n",
        "from sklearn.pipeline import Pipeline\n",
        "# Used for automating processes\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "# Used for spliting test and tuning hyperparameter\n",
        "from sklearn.metrics import accuracy_score\n",
        "# Used for calculating the performance\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "# Used for k-fold validiator in multi-classficiation\n",
        "\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "9TbRS-hBXsHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load data from files"
      ],
      "metadata": {
        "id": "bTgWau1hdl23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/EDFT_data/\n",
        "\n",
        "# This method use to extract all the file in the input list\n",
        "def extract_data(files):\n",
        "  data_x_raw = list()\n",
        "  data_y_raw = list()\n",
        "  for file in files:\n",
        "    with open(file) as f:\n",
        "      for line in f:\n",
        "        line = line.strip('\\n')\n",
        "        x_raw, y_raw = line.split(sep=';')\n",
        "\n",
        "        data_x_raw.append(x_raw)\n",
        "        data_y_raw.append(y_raw)\n",
        "\n",
        "  return data_x_raw, data_y_raw\n",
        "\n",
        "# Get all data\n",
        "data_train_x_raw, data_train_y_raw = extract_data(['train.txt', 'val.txt'])\n",
        "data_test_x_raw, data_test_y_raw = extract_data(['test.txt'])\n",
        "\n",
        "print(\"Traning data's size is:\", len(data_train_x_raw))\n",
        "print(\"Test data's size is:\", len(data_test_x_raw))"
      ],
      "metadata": {
        "id": "BY138Gindon3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Extraction\n"
      ],
      "metadata": {
        "id": "-UhsCl8HgxKV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing the stopword (stop word list given at https://countwordsfree.com/stopwords)\n"
      ],
      "metadata": {
        "id": "MkacoVpjhG6O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file = open(\"stop_words_english.txt\", 'r')\n",
        "stopword_list = file.read().split('\\n')\n",
        "file.close()\n",
        "\n",
        "# Due to the fact that the word \"feel\" takes too much space...\n",
        "stopword_list.append(\"feel\")\n",
        "stopword_list.append(\"feeling\")\n",
        "stopword_list.append(\"time\")\n",
        "stopword_list.append(\"bit\")\n",
        "stopword_list.append(\"ive\")\n",
        "stopword_list.append(\"im\")"
      ],
      "metadata": {
        "id": "0OCse4nNgaoy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vectorize the data:\n",
        "\n",
        "1.   Bag Of Words model (https://en.wikipedia.org/wiki/Bag-of-words_model)\n",
        "2.   BoW/TF-IDF model (https://en.wikipedia.org/wiki/Tf%E2%80%93idf)\n",
        "\n"
      ],
      "metadata": {
        "id": "kRGC3YwhD6Qx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scikit CountVectorizer:\n",
        "# Documentation: (https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)\n",
        "\n",
        "count_vector = CountVectorizer(stop_words=stopword_list, ngram_range=(1, 2))\n",
        "data_train_x_bow = count_vector.fit_transform(data_train_x_raw)\n",
        "dictionary = count_vector.get_feature_names_out()\n",
        "\n",
        "print(\"Shape of preprocessed training data X using BoW model is: \", data_train_x_bow.shape)"
      ],
      "metadata": {
        "id": "YY6UaDwAD_Aj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scikit TF-IDF:\n",
        "# Documentation: (https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer)\n",
        "\n",
        "tfidf_vector = TfidfVectorizer(stop_words=stopword_list, ngram_range=(1, 2))\n",
        "data_train_x_tfidf = tfidf_vector.fit_transform(data_train_x_raw)\n",
        "\n",
        "print(\"Shape of preprocessed training data X using BoW/TF-IDF model is: \", data_train_x_tfidf.shape)"
      ],
      "metadata": {
        "id": "D3a48rJjEHcG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cast output list to ndarray"
      ],
      "metadata": {
        "id": "fyttkaIPIuTR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_train_y = np.asarray(data_train_y_raw)\n",
        "print(\"Preprocessed training data y is:\", data_train_y)"
      ],
      "metadata": {
        "id": "jSQpqMqlI0Z9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Luckily we're quite done with the abbreviations!*"
      ],
      "metadata": {
        "id": "4ob9acTnuVLG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another way: Using Word2Vec to capture the semantics\n",
        "(Um, we'd talk about that later, ok?)\n"
      ],
      "metadata": {
        "id": "cqwxPqeLvqH8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uS4maCj-vvCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploratory Data Analysis (EDA)"
      ],
      "metadata": {
        "id": "0Fhcc_aUTueC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Average length of the comment:"
      ],
      "metadata": {
        "id": "2B0kWS9bktj2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate average number of characters\n",
        "sum_of_chars = sum([len(x) for x in data_train_x_raw])\n",
        "mean_number_chars = sum_of_chars / len(data_train_x_raw)\n",
        "\n",
        "# Calculate average number of words\n",
        "sum_of_words = sum([len(x.split()) for x in data_train_x_raw])\n",
        "mean_number_words = sum_of_words / len(data_train_x_raw)\n",
        "\n",
        "# Print\n",
        "print('The mean number of characters of each line is: %d' % mean_number_chars)\n",
        "print('The mean number of words of each line is: %d' % mean_number_words)"
      ],
      "metadata": {
        "id": "3n36QobPkewe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Statistic of output labels:"
      ],
      "metadata": {
        "id": "VAZxbqwQpJ_H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of labels:\n",
        "labels = np.unique(data_train_y)\n",
        "print('Number of labels is: %d' % len(labels))\n",
        "print('Labels: ' + ', '.join(labels))\n",
        "print('\\n----------------------------------------------------------------------------\\n')\n",
        "\n",
        "# Plot:\n",
        "df = pd.DataFrame(\n",
        "  dict(\n",
        "    labels = labels,\n",
        "    label_elements_count = [data_train_y_raw.count(label) for label in labels]\n",
        "  )\n",
        ")\n",
        "df = df.sort_values('label_elements_count')\n",
        "\n",
        "plt.title(label='Labels statistics')\n",
        "plt.xlabel('Labels')\n",
        "plt.ylabel('Count')\n",
        "\n",
        "plt.bar('labels', 'label_elements_count', data=df)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uqs1jJTspQX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Statistics of words with the highest frequency of each label:"
      ],
      "metadata": {
        "id": "BNNYNBrIts1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vect_arr = data_train_x_bow.toarray()\n",
        "vect_aggr = np.zeros(shape=(len(labels), vect_arr.shape[1]))\n",
        "\n",
        "for idx in range(len(vect_arr)):\n",
        "  label_idx = int(np.where(labels==data_train_y[idx])[0][0])\n",
        "\n",
        "  vect_aggr[label_idx] += vect_arr[idx]"
      ],
      "metadata": {
        "id": "eieb8hWmtsT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(3, 2)\n",
        "\n",
        "for label_idx in range(len(labels)):\n",
        "  axs_x, axs_y = label_idx // 2, label_idx % 2\n",
        "\n",
        "  plt_ref = axs[axs_x][axs_y]\n",
        "  plt_ref.set_title(labels[label_idx].upper())\n",
        "\n",
        "  x_axis, y_axis = list(), list()\n",
        "  n_argmax = np.argpartition(vect_aggr[label_idx], -5)[-5:]\n",
        "  n_argmax = n_argmax[np.argsort(vect_aggr[label_idx][n_argmax])]\n",
        "  for i in n_argmax:\n",
        "    x_axis.append(dictionary[i])\n",
        "    y_axis.append(vect_aggr[label_idx][i])\n",
        "\n",
        "  x_axis = np.asarray(x_axis)\n",
        "  y_axis = np.asarray(y_axis)\n",
        "\n",
        "  plt_ref.barh(x_axis, y_axis)\n",
        "\n",
        "# Adjust layout\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-VDZBpa-0ces"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Variables summary"
      ],
      "metadata": {
        "id": "9syl2WgjTppC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**data_train_x_raw**, **data_train_y_raw**: ***list***\n",
        "\n",
        "Raw training data which is extracted directly from files.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**data_test_x_raw**, **data_test_y_raw**: ***list***\n",
        "\n",
        "  Raw testing data which is extracted directly from files.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**stopword_list**: ***list***\n",
        "\n",
        " List of stop words used in BoW and BoW/TF-IDF models\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**dictionary**: ***numpy.ndarray***\n",
        "\n",
        " Dictionary of words which is used to vectorized the data and number of occurences.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**data_train_x_bow**: ***scipy.sparse._csr.csr_matrix***\n",
        "\n",
        " Trainning data input which is vectorized by the BoW model.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**data_train_x_tfidf**: ***scipy.sparse._csr.csr_matrix***\n",
        "\n",
        " Training data input which is vectorized by the BoW/TFIDF model\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**data_train_y**: ***numpy.ndarray***\n",
        "\n",
        " Preprocessed traning data output\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "rvyhe6bnT0at"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model training and result**"
      ],
      "metadata": {
        "id": "xgd9jGkTFGQF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##General"
      ],
      "metadata": {
        "id": "lSnAfK_eZm1d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We train some models from simple to complex:\n",
        "\n",
        "\n",
        "*   K-nearest neighbors (KNN)\n",
        "*   Naive Bayes Classification\n",
        "*   K-means clusters\n",
        "*   Neural network\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6ol81Oa6Zoxn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## K-nearest neighbors (KNN)"
      ],
      "metadata": {
        "id": "GhK_uRQbeKiA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Naive Bayes classification"
      ],
      "metadata": {
        "id": "bhO2ye_2eS8d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "XPH1AE2JFN4y"
      }
    }
<<<<<<< Updated upstream
  ]
}
=======
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
>>>>>>> Stashed changes
