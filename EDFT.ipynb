{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/auphong2707/emotion-dectection-from-text/blob/main/EDFT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction"
      ],
      "metadata": {
        "id": "AHEsUwSBCHbK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The project aims to create a **Machine Learning** powered model which is capable of classifying the emotions of the text (in the form of short quotes/comments) with predefined labels of emotions."
      ],
      "metadata": {
        "id": "FRPrD_ccFVz9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Emotion detection, also known as sentiment analysis, is now one of the most attractive subfields of Machine Learning, especially Natural Language Processing (NLP) due to its wide applications in many aspects of modern life, such as providing emotional information to help people developing insights or making decisions. In this project, we specifically try to convey the main emotion of a comment sentence. Generally, we expect the model to be able to take the input sentence in the form of short comments and returns its major emotion(s), corresponding to one of the predefined labels."
      ],
      "metadata": {
        "id": "TMh_xASYG7cZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our approach to solving this problem is to split it into two halves:\n",
        "\n",
        "  1.   Data Preprocessing\n",
        "  2.   Model training and Result\n",
        "\n",
        "For detail of each half, please go into the section\n"
      ],
      "metadata": {
        "id": "T65Wg0STHA3s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset we use in this project is obtained from Kaggle: [EDFT Dataset](https://www.kaggle.com/datasets/praveengovi/emotions-dataset-for-nlp)"
      ],
      "metadata": {
        "id": "fDK4XIuNIgMl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Connect to Google Drive"
      ],
      "metadata": {
        "id": "6CJSmsQlUZJt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Preprocessing**"
      ],
      "metadata": {
        "id": "nj7DW4_mFDBh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importation & Data loading"
      ],
      "metadata": {
        "id": "aDTriPXukNqb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "2w5klLSgUYgY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0fe06ac-9f40-4d50-e3af-1a19fa1ab43b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preparing necessary packages (may need to add more):"
      ],
      "metadata": {
        "id": "NKqQ0TvIZSOK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install word2vec\n",
        "!pip install gensim\n",
        "\n",
        "import numpy as np  # Work with multi-dimenional data\n",
        "import pandas as pd # Work with relational data\n",
        "import matplotlib.pyplot as plt # Visualize data\n",
        "import seaborn as sns # Visualize data base on matplotlib\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "# Because I hate seeing stupid warnings~\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
        "# Used for automatic feature extraction\n",
        "from sklearn.pipeline import Pipeline\n",
        "# Used for automating processes"
      ],
      "metadata": {
        "id": "9TbRS-hBXsHe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52cb9828-787f-4ea1-fa0d-b14f6e10b3cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting word2vec\n",
            "  Downloading word2vec-0.11.1.tar.gz (42 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load data from files"
      ],
      "metadata": {
        "id": "bTgWau1hdl23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/EDFT_data/\n",
        "\n",
        "# Get all data\n",
        "files = ['train.txt', 'val.txt']\n",
        "\n",
        "X_raw = list()\n",
        "Y_raw = list()\n",
        "for file in files:\n",
        "  f = open(file, 'r')\n",
        "  for line in f:\n",
        "    line = line.strip('\\n')\n",
        "    x_raw, y_raw = line.split(sep=';')\n",
        "\n",
        "    X_raw.append(x_raw)\n",
        "    Y_raw.append(y_raw)\n",
        "  f.close()"
      ],
      "metadata": {
        "id": "BY138Gindon3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Extraction\n"
      ],
      "metadata": {
        "id": "-UhsCl8HgxKV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing the stopword (stop word list given at https://countwordsfree.com/stopwords)\n"
      ],
      "metadata": {
        "id": "MkacoVpjhG6O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file = open(\"stop_words_english.txt\", 'r')\n",
        "stopword_list = file.read().split('\\n')\n",
        "file.close()"
      ],
      "metadata": {
        "id": "0OCse4nNgaoy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# These codes do the same things\n",
        "\n",
        "# Handcode (edited to return bigrams as well)\n",
        "dictionary = set()\n",
        "c = 0\n",
        "for quoted in X_raw:\n",
        "  c += 1\n",
        "  if c == 2:\n",
        "    print(quoted)\n",
        "  quote = ' ' + quoted + ' '\n",
        "  for stopword in stopword_list:\n",
        "    quote = ' '.join(quote.split((' '+stopword+' ')))\n",
        "  quote = quote.split()\n",
        "  for i in range(len(quote)):\n",
        "    dictionary.add(quote[i])\n",
        "    if i != 0:\n",
        "      dictionary.add(' '.join([quote[i], quote[i-1]]))\n",
        "    break\n",
        "\n",
        "# Scikit (using CountVectorizer)\n",
        "count_vector = CountVectorizer(stop_words=stopword_list, ngram_range=(1, 2))\n",
        "bow = count_vector.fit_transform(X_raw)\n",
        "dictionary2 = count_vector.get_feature_names_out()\n",
        "for i in bow[1].nonzero()[1]:\n",
        "  print(count_vector.get_feature_names_out()[i])\n",
        "# bow already outputs the BoW array.\n",
        "# If you want to output the TF-IDF array, use TfidfVectorizer instead.\n",
        "\n",
        "# count_vector.vocabulary_.keys()\n",
        "\n",
        "tfidf_vector = TfidfVectorizer(stop_words=stopword_list, ngram_range=(1, 2))\n",
        "tfidf = tfidf_vector.fit_transform(X_raw)\n",
        "\n"
      ],
      "metadata": {
        "id": "lGO28dvenYmI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Luckily we're quite done with the abbreviations!*"
      ],
      "metadata": {
        "id": "4ob9acTnuVLG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another way: Using Word2Vec to capture the semantics\n",
        "(Um, we'd talk about that later, ok?)\n"
      ],
      "metadata": {
        "id": "cqwxPqeLvqH8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uS4maCj-vvCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploratory Data Analysis (EDA)"
      ],
      "metadata": {
        "id": "0Fhcc_aUTueC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Average length of the comment:"
      ],
      "metadata": {
        "id": "2B0kWS9bktj2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate average number of characters\n",
        "sum_of_chars = sum([len(x) for x in X_raw])\n",
        "mean_number_chars = sum_of_chars / len(X_raw)\n",
        "\n",
        "# Calculate average number of words\n",
        "sum_of_words = sum([len(x.split()) for x in X_raw])\n",
        "mean_number_words = sum_of_words / len(X_raw)\n",
        "\n",
        "# Print\n",
        "print('The mean number of characters of each line is: %d' % mean_number_chars)\n",
        "print('The mean number of words of each line is: %d' % mean_number_words)"
      ],
      "metadata": {
        "id": "3n36QobPkewe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Statistic of output labels:"
      ],
      "metadata": {
        "id": "VAZxbqwQpJ_H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of labels:\n",
        "labels = list(set(Y_raw))\n",
        "print('Number of labels is: %d' % len(labels))\n",
        "print('Labels: ' + ', '.join(labels))\n",
        "print('\\n----------------------------------------------------------------------------\\n')\n",
        "\n",
        "# Plot:\n",
        "label_elements_count = np.asarray([Y_raw.count(label) for label in labels])\n",
        "labels = np.asarray(labels)\n",
        "\n",
        "plt.title(label='Labels statistics')\n",
        "plt.xlabel('Labels')\n",
        "plt.ylabel('Count')\n",
        "\n",
        "plt.bar(x=labels, height=label_elements_count)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uqs1jJTspQX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Statistics of words with the highest frequency of each label:"
      ],
      "metadata": {
        "id": "BNNYNBrIts1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vect_arr = bow.toarray()\n",
        "vect_aggr = np.zeros(shape=(len(labels), vect_arr.shape[1]))\n",
        "\n",
        "for idx in range(len(vect_arr)):\n",
        "  label_idx = int(np.where(labels==Y_raw[idx])[0][0])\n",
        "\n",
        "  vect_aggr[label_idx] += vect_arr[idx]"
      ],
      "metadata": {
        "id": "eieb8hWmtsT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for label_idx in range(len(labels)):\n",
        "  print(labels[label_idx].upper() + '\\n')\n",
        "  n_argmax = np.argpartition(vect_aggr[label_idx], -10)[-10:]\n",
        "  n_argmax = n_argmax[np.argsort(vect_aggr[label_idx][n_argmax])]\n",
        "  for i in n_argmax:\n",
        "    print(dictionary2[i], vect_aggr[label_idx][i])\n",
        "\n",
        "  print('-------------------')"
      ],
      "metadata": {
        "id": "-VDZBpa-0ces"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Popular stop words"
      ],
      "metadata": {
        "id": "Q-UeGTpH2AGL"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1Xc615iV2EXW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model training and result**"
      ],
      "metadata": {
        "id": "xgd9jGkTFGQF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##General"
      ],
      "metadata": {
        "id": "lSnAfK_eZm1d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We train some models from simple to complex:\n",
        "\n",
        "\n",
        "*   K-nearest neighbors (KNN)\n",
        "*   Naive Bayes Classification\n",
        "*   K-means clusters\n",
        "*   Neural network\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6ol81Oa6Zoxn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## K-nearest neighbors (KNN)"
      ],
      "metadata": {
        "id": "GhK_uRQbeKiA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Naive Bayes classification"
      ],
      "metadata": {
        "id": "bhO2ye_2eS8d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "XPH1AE2JFN4y"
      }
    }
  ]
}